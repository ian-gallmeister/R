---
title: "Class Notes"
author: "Statistical Computing & Machine Learning"
date: "Class 15"
output: rmarkdown::tufte_handout
---

```{r include=FALSE}
require(mosaic)
require(ISLR)
knitr::opts_chunk$set(tidy=FALSE)
```

# Efron article

Walk through Brad Efron's "Thinking the unthinkable" article. (In `Efron_Bootstrapping_Article.pdf`.)

* On first page, he states that the assumption of a normal distribution is the biggest flaw in a t-test.  But what about covariates?
* He shows bootstrapping, cross-validation, and LDA.

Re-create the jacknife calculation in a more modern way. See `gtools::combinations(16, 7)` to get all possible combinations of the indicies to fill in the size-7 group.  Then loop over these and to find all the different difference-in-means examples from the data he provided.

We could approximate this by randomization.

Show a bootstrap calculation in `mosaic` form. Example: difference in widths of boys and girls feet.

```{r eval = FALSE}
require(mosaic)
lm(width ~ sex, data = KidsFeet)
trials1 <- do(500) * lm(width ~ sex, data = resample(KidsFeet))
# discuss the confidence interval on the sexG coefficient

# Show that the result is quite different if length is added as a covariate.
trials2 <- do(500) * lm(width ~ sex + length, data = resample(KidsFeet))

```

* He doesn't have any code examples.  Why not?
* He doesn't have any data in machine readable form. Why not?



# Programming

* Basic Types
    - numeric
    - character
    - list ... data.frame
    - matrix
    - vector
    - function
    - formula
* Flow
    - functions
        - return(x)
        - ...
    - conditionals
    - loops
    - `stop()` exceptions
    - non-standard evaluation
        - lazy evaluation
        - parsing
        

Functionals.  How `optim()` allows you to pass the various other arguments, e.g. `data=` to the function being optimized.

The meaning of `...`

Solvers

# Communicating with software.

Basic question: what fraction of the original sample is represented in a bootstrap sample.

Work toward developing a complete and effective style of communicating with software.  Work with class to evolve a function:

1. A simple calculation of the fraction of the original contained in a bootstrap sample.  Run this with up arrow several times.
2. Add a loop --- accumulator, body, package up result.
3. Encapsulate the code in a function --- give reasonable defaults for the arguments.
4. Take out the central logic in the loop and implement as a helper function.  Initially, define the helper inside the function.
5. Add documentation in the form of Roxygen comments
6. Place this into a package.

End result: something like `bootstrap_fraction.R`.



# Bootstrapping


\includegraphics[width=\textwidth]{../ISL-Book-Figures/Chapter5/{5.10}.pdf}
\marginnote{ISLR Figure 5.10}

In-class demonstration.  How many cases don't get used in a typical resample?



```{r}
cases <- 1:10000
ntrials <- 50
result <- numeric(ntrials)
for (k in 1:50) {
  bootstrap_sample <- sample(cases, size=length(cases), replace = TRUE)
  result[k] <- length(setdiff(cases, unique(bootstrap_sample))) / length(cases)
}
result

## Or ...
(1 - 1/length(cases))^length(cases)
```

**Programming a bootstrap calculation**

```{r}
bootstrap <- function(formula, method, data, statistic, reps=10) {
  
  
}
```

**Mosaic software**

Using `resample()` and `do()`


# Model Selection

Problem: Given a set of potential model terms, choose the best subset.

Two issues:

1. What does "best" mean?
2. How to optimize?

## Best

In-sample  |  Adjusted   | Out-of-sample
-----------|-------------|---------------
$\frac{1}{n}$RSS | $C_p = \frac{1}{n}(\mbox{RSS} + 2 d \hat{\sigma}^2)$       | cross-validated prediction error
. | $\mbox{AIC} = -2 \ln {\cal L} - 2 d$ | .
. | $\mbox{AIC}_{ls} = \frac{1}{\hat{\sigma}^2}C_p$ | .
. | $\mbox{BIC} = \frac{1}{n} (\mbox{RSS} + \ln(n) d \hat{\sigma}^2)$ | .
R$^2$ | Adjusted R$^2$ | ???

$\mbox{Adjusted R}^2 = 1 - \frac{\mbox{RSS}/(n-d-1)}{\mbox{TSS}{(n-1)}}$

\marginnote{ISLR Figure 6.2.  Note that the values on the vertical axis are the best for that "number of predictors."}
\includegraphics[width=\textwidth]{../ISL-Book-Figures/Chapter6/{6.2}.pdf}

**Uncertainty**

Repeat the analysis for different test sets or using different folds in k-fold cross validation.

* At each value of "Number of Predictors", there will be a distribution.
* *One-standard-error rule*: select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.

# Optimization

## What are we optimizing over?

Choose the best set of columns from the model matrix.

```{r}
Small <- mosaic::sample(mosaicData::KidsFeet, size = 5)
row.names(Small) <- NULL
M1 <- model.matrix(width ~ length + sex, data = Small)
M2 <- model.matrix(width ~ length + sex*biggerfoot, data = Small)
M3 <- model.matrix(width ~ length*domhand*sex + sex*biggerfoot, data = Small)
```

Compare ranks.

Use `tmp <- qr(M5)` and `qr.Q(tmp)` and `qr.R(tmp)` to show how to find the smallest matrix of that rank.

## Techniques for optimization

**Exhaustion**

Compare all the $2^d$ possible choices.

**Greedy algorithms**

For term selection ...

- Start with the intercept. At each step, add the single term that most increases the figure of merit.
- Start with the full model.  At each step, delete the single term that least decreases the figure of merit.
    
Two problems:

1. Egg carton problem
2. Finite step and the gradient not pointing to the top of the mountain.  (Draw a narrow set of nested ellipses.  The steepest direction is orthogonal to the level curves, not likely to be pointing to the top.)

**Not so greedy**

Choose the best model with exactly $d$ vectors.

Increase $d$ upward until the figure of merit gets worse.

Programming:
```{r}
gtools::combinations(5,3)
```

# Thursday: Shrinkage methods

# Tuesday: Principal Components

# Daily activity

Write a k-fold cross validator. Day-13-Programming-Tasks
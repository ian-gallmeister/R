---
title: "Topic 4 Assignment"
author: "Ian Gallmeister"
date: ""
output: 
  html_document:
    fig_height: 3
    fig_width: 5
---
<!-- Don't edit in between this line and the one below -->
```{r include=FALSE}
# Don't delete this chunk if you are using the DataComputing package
library(DataComputing)
```
*Source file* 
```{r, results='asis', echo=FALSE}
includeSourceDocuments()
```
<!-- Don't edit the material above this line -->


##4.7.1  
Show $p(x) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}$ is the same as $\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1 X}$
$$
\frac{p(X)}{1 - p(X)} = \frac{\frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}}{1 - \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}} = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X} - e^{\beta_0 + \beta_1 X}} = \frac{e^{\beta_0 + \beta_1 X}}{1} = e^{\beta_0 + \beta_1 X}
$$

#4.7.8

I would personally use the logistic regression because KNN has the potential to classify an entire dataset as the same thing from one point, plus we don't know how different the test and training data for KNN is.

#4.7.9  
**(a)** Odds = $\frac{0.37}{1.00} \Rightarrow$ P = $\frac{0.37}{1.37} = 0.27$  
**(b)** P = $\frac{16}{100} \Rightarrow$ Odds = $\frac{16}{1 - 16} = \frac{16}{84} = 0.19$

#4.7.11
```{r}
#(a)
data(Auto, package="ISLR")
mpg01 <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
Auto2 <- data.frame(Auto, mpg01)

#(b)
plot(mpg01, Auto$horsepower)
plot(mpg01, Auto$displacement)
plot(mpg01, Auto$weight)
plot(mpg01, Auto$acceleration)
plot(mpg01, Auto$year)
plot(mpg01, Auto$cylinders)

#(c)
nrow(Auto)
test <- sample(392, 196)
testAuto <- Auto2[test,]
trainAuto <- Auto2[setdiff(1:392, test),]

#(d)
library(MASS)
lad <- lda(trainAuto$mpg01 ~ trainAuto$horsepower + trainAuto$weight)
lad
ldaprediction <- predict(lad, testAuto)
predictionclass <- ldaprediction$class
table(predictionclass)
mean(predictionclass == testAuto$mpg01)
#So it's accurate 49.5% of the time

#(e)
qad <- qda(trainAuto$mpg01 ~ trainAuto$horsepower + trainAuto$weight)
qad
qadclass <- predict(qad, testAuto)$class
table(qadclass)
mean(qadclass == testAuto$mpg01)
#So this is accurate 50% of the time

#(f)
logistic <- glm(trainAuto$mpg01 ~ trainAuto$horsepower + trainAuto$weight, family=binomial)
summary(logistic)
probability <- predict(logistic, testAuto, type="response")
prediction <- rep(1, 196)
prediction[probability<0.5] <- 0
mean(prediction == testAuto$mpg01)
#So this is accurate 48.4% of the time

#(g)
library(class)
trainAuto2 <- cbind(trainAuto$horsepower, trainAuto$weight)
testAuto2 <- cbind(testAuto$horsepower, testAuto$weight)
set.seed(1)
knnprediction <- knn(trainAuto2, testAuto2, trainAuto$mpg01, k=1)
table(knnprediction)
mean(knnprediction == testAuto$mpg01)
#Accurate 88% of the time
knnprediction5 <- knn(trainAuto2, testAuto2, trainAuto$mpg01, k=5)
table(knnprediction5)
mean(knnprediction5 == testAuto$mpg01)
#Accurate 87% of the time.  Roughly comparable.
knnprediction10 <- knn(trainAuto2, testAuto2, trainAuto$mpg01, k=10)
table(knnprediction10)
mean(knnprediction10 == testAuto$mpg01)
#Also accurate about 87% of the time.  Seems to be a theme.
```


#4.7.13

```{r, fig.height=7, fig.width=7}
library(MASS) #LDA and data
library(class) #KNN

data(Boston)
pairs(Boston)
#lstat, age, nox, ptratio, tax

crime01 <- ifelse(Boston$crim > median(Boston$crim), 1, 0)
Boston2 <- data.frame(Boston, crime01)
train <- sample(nrow(Boston), 0.5*nrow(Boston))
test <- setdiff(1:nrow(Boston), train)
trainBoston <- Boston2[train,]
testBoston <- Boston2[test,]


#Logistic
log <- glm(crime01 ~ lstat + age + nox + ptratio + tax, data=trainBoston, family=binomial)
summary(log)

log2 <- glm(crime01 ~ nox + rm, data=trainBoston, family=binomial)
summary(log2)
chances <- predict(log2, type="response")
predictions <- rep(0, nrow(trainBoston))
predictions[chances>0.5] <- 1
table(predictions, testBoston$crime01)
mean(predictions == testBoston$crime01)
#51% accuracy

#LDA
lad <- lda(crime01 ~ nox + rm, data = trainBoston)
lad
chances <- predict(lad, testBoston)
table(chances$class, testBoston$crime01)
mean(chances$class == testBoston$crime01)
#85% accuracy

#KNN
trainBoston2 <- cbind(trainBoston$nox, trainBoston$rm)
testBoston2 <- cbind(testBoston$nox, testBoston$rm)
set.seed(1)

knnprediction3 <- knn(trainBoston2, testBoston2, testBoston$crime01, k=3)
table(knnprediction3)
mean(knnprediction3 == testBoston$crime01)
#57% accuracy

knnprediction5 <- knn(trainBoston2, testBoston2, testBoston$crime01, k=5)
table(knnprediction5)
mean(knnprediction5 == testBoston$crime01)
#54% accuracy

knnprediction10 <- knn(trainBoston2, testBoston2, testBoston$crime01, k=10)
table(knnprediction10)
mean(knnprediction10 == testBoston$crime01)
#57% accuracy

knnprediction30 <- knn(trainBoston2, testBoston2, testBoston$crime01, k=30)
table(knnprediction30)
mean(knnprediction30 == testBoston$crime01)
#56% accuracy
```